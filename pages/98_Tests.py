"""
Page Tests - Interface pour lancer les tests unitaires et d'intÃ©gration
"""
import streamlit as st
import subprocess
import sys
import os
from pathlib import Path
import json
import time

# Configuration de la page
st.set_page_config(
    page_title="Tests - FinancePerso",
    page_icon="ğŸ§ª",
    layout="wide"
)

st.title("ğŸ§ª Tests & Quality")

# Tabs pour organiser l'interface
tab1, tab2, tab3 = st.tabs(["ğŸš€ Lancer Tests", "ğŸ“Š RÃ©sultats", "ğŸ“š Documentation"])

with tab1:
    st.header("Lancer les Tests")
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.subheader("CatÃ©gorie de Tests")
        
        test_category = st.radio(
            "SÃ©lectionnez les tests Ã  lancer :",
            [
                "ğŸŒŸ Tous les tests (197 tests)",
                "ğŸ’¾ Tests DB (108 tests)",
                "ğŸ§  Tests AI (8 tests)",
                "ğŸ¨ Tests UI Components (54 tests)",
                "ğŸ”„ Tests Integration (9 tests)",
                "âš™ï¸ Tests Logic (18 tests)"
            ],
            help="Choisissez quelle catÃ©gorie de tests vous souhaitez exÃ©cuter"
        )
        
        # Options avancÃ©es
        with st.expander("âš™ï¸ Options AvancÃ©es"):
            verbose = st.checkbox("Mode verbose (-vv)", value=False)
            stop_on_first = st.checkbox("ArrÃªter au premier Ã©chec (-x)", value=False)
            show_output = st.checkbox("Afficher les print() (-s)", value=False)
            coverage = st.checkbox("Calculer la coverage (--cov)", value=False)
            
            keyword_filter = st.text_input(
                "Filtrer par mot-clÃ©",
                placeholder="Ex: 'budget', 'transaction', 'duplicate'...",
                help="Lancer uniquement les tests contenant ce mot-clÃ©"
            )
    
    with col2:
        st.subheader("Actions")
        
        if st.button("â–¶ï¸ Lancer les Tests", type="primary", use_container_width=True):
            # Construction de la commande
            cmd = ["python3", "-m", "pytest"]
            
            # Ajouter la catÃ©gorie
            if "Tous" in test_category:
                pass  # Pas de filtre
            elif "DB" in test_category:
                cmd.append("db/")
            elif "UI" in test_category:
                cmd.append("ui/")
            elif "Integration" in test_category:
                cmd.append("test_integration.py")
            elif "AI" in test_category:
                cmd.append("ai/")
            elif "Logic" in test_category:
                cmd.extend(["test_utils.py", "test_analytics.py"])
            
            # Options
            if verbose:
                cmd.append("-vv")
            else:
                cmd.append("-v")
            
            if stop_on_first:
                cmd.append("-x")
            
            if show_output:
                cmd.append("-s")
            
            if coverage:
                cmd.extend(["--cov=modules", "--cov-report=term-missing"])
            
            if keyword_filter:
                cmd.extend(["-k", keyword_filter])
            
            # Ignorer .DS_Store
            cmd.extend(["--ignore=../.DS_Store", "--ignore=.DS_Store"])
            
            # Lancer les tests
            with st.spinner("ğŸ”„ ExÃ©cution des tests en cours..."):
                start_time = time.time()
                
                try:
                    # Changer vers le dossier tests
                    result = subprocess.run(
                        cmd,
                        cwd=os.path.join(os.getcwd(), "tests"),
                        capture_output=True,
                        text=True,
                        timeout=120
                    )
                    
                    execution_time = time.time() - start_time
                    
                    # Stocker les rÃ©sultats en session state
                    st.session_state['last_test_result'] = {
                        'stdout': result.stdout,
                        'stderr': result.stderr,
                        'returncode': result.returncode,
                        'execution_time': execution_time,
                        'command': ' '.join(cmd)
                    }
                    
                    # Afficher rÃ©sumÃ©
                    if result.returncode == 0:
                        st.success(f"âœ… Tests rÃ©ussis en {execution_time:.2f}s !")
                    else:
                        st.error(f"âŒ Certains tests ont Ã©chouÃ© (code: {result.returncode})")
                    
                    # Switch vers l'onglet rÃ©sultats
                    st.info("ğŸ‘‰ Consultez l'onglet 'RÃ©sultats' pour voir les dÃ©tails")
                    
                except subprocess.TimeoutExpired:
                    st.error("â±ï¸ Timeout: Les tests ont pris plus de 2 minutes")
                except Exception as e:
                    st.error(f"âŒ Erreur lors de l'exÃ©cution: {e}")
        
        if st.button("ğŸ—‘ï¸ Nettoyer les caches", use_container_width=True):
            # Nettoyer les caches pytest
            cache_dirs = [
                os.path.join("tests", ".pytest_cache"),
                os.path.join("tests", "__pycache__"),
            ]
            
            cleaned = 0
            for cache_dir in cache_dirs:
                if os.path.exists(cache_dir):
                    import shutil
                    shutil.rmtree(cache_dir)
                    cleaned += 1
            
            st.success(f"âœ… {cleaned} cache(s) nettoyÃ©(s)")

with tab2:
    st.header("ğŸ“Š RÃ©sultats des Tests")
    
    if 'last_test_result' in st.session_state:
        result = st.session_state['last_test_result']
        
        # MÃ©triques
        col1, col2, col3 = st.columns(3)
        
        with col1:
            status_icon = "âœ…" if result['returncode'] == 0 else "âŒ"
            st.metric("Status", f"{status_icon} {'Success' if result['returncode'] == 0 else 'Failed'}")
        
        with col2:
            st.metric("Temps d'exÃ©cution", f"{result['execution_time']:.2f}s")
        
        with col3:
            st.metric("Code de sortie", result['returncode'])
        
        st.divider()
        
        # Commande exÃ©cutÃ©e
        st.subheader("Commande ExÃ©cutÃ©e")
        st.code(result['command'], language="bash")
        
        # Output
        st.subheader("Sortie des Tests")
        
        output = result['stdout'] + result['stderr']
        
        # Parser pour extraire des infos
        if "passed" in output:
            # Extraire le rÃ©sumÃ©
            lines = output.split('\n')
            for line in lines:
                if 'passed' in line or 'failed' in line or 'error' in line:
                    if '=====' in line:
                        st.info(line.strip('=').strip())
        
        # Output complet
        with st.expander("ğŸ“ Output Complet", expanded=True):
            st.text_area(
                "Sortie complÃ¨te",
                value=output,
                height=400,
                label_visibility="collapsed"
            )
        
        # Bouton de tÃ©lÃ©chargement
        st.download_button(
            label="ğŸ’¾ TÃ©lÃ©charger les rÃ©sultats",
            data=output,
            file_name=f"test_results_{int(time.time())}.txt",
            mime="text/plain"
        )
        
    else:
        st.info("ğŸ‘† Lancez d'abord des tests dans l'onglet 'ğŸš€ Lancer Tests'")
        
        # Afficher les stats du projet
        st.subheader("ğŸ“ˆ Statistiques du Projet")
        
        stats_col1, stats_col2, stats_col3 = st.columns(3)
        
        with stats_col1:
            st.metric("Tests Disponibles", "197")
            st.caption("108 DB + 8 AI + 54 UI + 18 Logic + 9 Integration")
        
        with stats_col2:
            st.metric("Coverage EstimÃ©e", "~75%")
            st.caption("Couverture globale du code")
        
        with stats_col3:
            st.metric("Modules TestÃ©s", "19")
            st.caption("Fichiers de test")

with tab3:
    st.header("ğŸ“š Documentation des Tests")
    
    st.markdown("""
    ## ğŸ¯ Organisation des Tests
    
    Les tests sont organisÃ©s en **3 catÃ©gories principales** :
    
    ### ğŸ’¾ Tests DB (108 tests)
    - **Transactions** (19) : CRUD, filtering, duplicates
    - **CatÃ©gories** (19) : CRUD, emojis, merging, ghosts
    - **Membres** (16) : CRUD, renaming, card mappings
    - **Tags** (13) : Normalisation, global delete
    - **Rules** (11) : Pattern matching, auto-categorization
    - **Budgets** (11) : CRUD, validation
    - **Audit** (8) : Orphans, transfers
    - **Stats** (8) : Analytics
    - **Migrations** (6) : Schema versioning
    
    ### ğŸ¨ Tests UI Components (54 tests)
    - **KPI Cards** (7) : Trends calculation
    - **Category Charts** (6) : Data aggregation
    - **Progress Tracker** (8) : Progress logic
    - **Tag Manager** (11) : Tag validation
    - **Member Selector** (10) : Filtering
    - **Filters** (12) : Date/category filtering
    
    ### ğŸ”„ Tests Integration (9 tests)
    - Import â†’ Categorization workflow
    - Validation â†’ Learning rules
    - Budget tracking
    - Audit workflows
    - End-to-end scenarios
    
    ---
    
    ## ğŸš€ Commandes CLI
    
    Si vous prÃ©fÃ©rez lancer les tests en ligne de commande :
    
    ```bash
    # Tous les tests
    cd tests && ./run_tests.sh
    
    # Tests DB uniquement
    cd tests && ./run_tests.sh db/
    
    # Tests UI uniquement
    cd tests && ./run_tests.sh ui/
    
    # Tests d'intÃ©gration
    cd tests && ./run_tests.sh test_integration.py
    
    # Avec coverage
    cd tests && ./run_tests.sh --cov=modules --cov-report=html
    ```
    
    ---
    
    ## âš ï¸ Note macOS
    
    Ã€ cause du problÃ¨me `.DS_Store` (SIP protection), utilisez :
    - Le script `run_tests.sh` fourni
    - Ou supprimez les `.DS_Store` : `find . -name ".DS_Store" -delete`
    
    ---
    
    ## ğŸ“– Plus d'infos
    
    Consultez `tests/README.md` pour la documentation complÃ¨te.
    """)
    
    # Lien vers README
    readme_path = os.path.join("tests", "README.md")
    if os.path.exists(readme_path):
        with open(readme_path, 'r') as f:
            readme_content = f.read()
        
        with st.expander("ğŸ“„ Voir tests/README.md"):
            st.markdown(readme_content)

# Footer
st.divider()

from modules.ui import render_scroll_to_top
render_scroll_to_top()

st.caption("ğŸ§ª Test Suite - 197 tests | ~78% coverage | Production Ready")
